{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cthb8O3LCPM1"
      },
      "source": [
        "# SynthID Text: Watermarking for Generated Text\n",
        "\n",
        "This notebook demonstrates how to use the [SynthID Text library][synthid-code]\n",
        "to apply and detect watermarks on generated text. It is divided into three major\n",
        "sections and intended to be run end-to-end.\n",
        "\n",
        "1.  **_Setup_**: Importing the SynthID Text library, choosing your model (either\n",
        "    [Gemma][gemma] or [GPT-2][gpt2]) and device (either CPU or GPU, depending\n",
        "    on your runtime), defining the watermarking configuration, and initializing\n",
        "    some helper functions.\n",
        "1.  **_Applying a watermark_**: Loading your selected model using the\n",
        "    [Hugging Face Transformers][transformers] library, using that model to\n",
        "    generate some watermarked text, and comparing the perplexity of the\n",
        "    watermarked text to that of text generated by the base model.\n",
        "1.  **_Detecting a watermark_**: Training a detector to recognize text generated\n",
        "    with a specific watermarking configuration, and then using that detector to\n",
        "    predict whether a set of examples were generated with that configuration.\n",
        "\n",
        "As the reference implementation for the\n",
        "[SynthID Text paper in _Nature_][synthid-paper], this library and notebook are\n",
        "intended for research review and reproduction only. They should not be used in\n",
        "production systems. For a production-grade implementation, check out the\n",
        "official SynthID logits processor in [Hugging Face Transformers][transformers].\n",
        "\n",
        "[gemma]: https://ai.google.dev/gemma/docs/model_card\n",
        "[gpt2]: https://huggingface.co/openai-community/gpt2\n",
        "[synthid-code]: https://github.com/google-deepmind/synthid-text\n",
        "[synthid-paper]: https://www.nature.com/\n",
        "[transformers]: https://huggingface.co/docs/transformers/en/index"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "be-I0MNRbyWT"
      },
      "source": [
        "# 1. Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "aq7hChW8njFo",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# @title Install and import the required Python packages\n",
        "#\n",
        "# @markdown Running this cell may require you to restart your session.\n",
        "\n",
        "! pip install synthid-text[notebook]\n",
        "\n",
        "from collections.abc import Sequence\n",
        "import enum\n",
        "import gc\n",
        "\n",
        "import datasets\n",
        "import huggingface_hub\n",
        "from synthid_text import detector_mean\n",
        "from synthid_text import logits_processing\n",
        "from synthid_text import synthid_mixin\n",
        "from synthid_text import detector_bayesian\n",
        "import tensorflow as tf\n",
        "import torch\n",
        "import tqdm\n",
        "import transformers\n",
        "!sudo apt update\n",
        "!sudo apt install -y pciutils\n",
        "!curl -fsSL https://ollama.com/install.sh | sh\n",
        "!pip install langchain-ollama\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_ollama.llms import OllamaLLM\n",
        "from IPython.display import Markdown\n",
        "import threading\n",
        "import subprocess\n",
        "import time\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w9a5nANolFS_",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Choose your model.\n",
        "#\n",
        "# @markdown This reference implementation is configured to use the Gemma v1.0\n",
        "# @markdown Instruction-Tuned variants in 2B or 7B sizes, or GPT-2.\n",
        "\n",
        "\n",
        "class ModelName(enum.Enum):\n",
        "  GPT2 = 'gpt2'\n",
        "  GEMMA_2B = 'google/gemma-2b-it'\n",
        "  GEMMA_7B = 'google/gemma-7b-it'\n",
        "\n",
        "\n",
        "model_name = 'google/gemma-2b-it' # @param ['gpt2', 'google/gemma-2b-it', 'google/gemma-7b-it']\n",
        "MODEL_NAME = ModelName(model_name)\n",
        "\n",
        "if MODEL_NAME is not ModelName.GPT2:\n",
        "  huggingface_hub.notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B_pe-hG6SW6H",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Configure your device\n",
        "#\n",
        "# @markdown This notebook loads models from Hugging Face Transformers into the\n",
        "# @markdown PyTorch deep learning runtime. PyTorch supports generation on CPU or\n",
        "# @markdown GPU, but your chosen model will run best on the following hardware,\n",
        "# @markdown some of which may require a\n",
        "# @markdown [Colab Subscription](https://colab.research.google.com/signup).\n",
        "# @markdown\n",
        "# @markdown * Gemma v1.0 2B IT: Use a GPU with 16GB of memory, such as a T4.\n",
        "# @markdown * Gemma v1.0 7B IT: Use a GPU with 32GB of memory, such as an A100.\n",
        "# @markdown * GPT-2: Any runtime will work, though a High-RAM CPU or any GPU\n",
        "# @markdown   will be faster.\n",
        "\n",
        "DEVICE = (\n",
        "    torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n",
        ")\n",
        "DEVICE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UOGvCjyVjjQ5",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Example watermarking config\n",
        "#\n",
        "# @markdown SynthID Text produces unique watermarks given a configuration, with\n",
        "# @markdown the most important piece of a configuration being the `keys`: a\n",
        "# @markdown sequence of unique integers.\n",
        "# @markdown\n",
        "# @markdown This reference implementation uses a fixed watermarking\n",
        "# @markdown configuration, which will be displayed when you run this cell.\n",
        "\n",
        "CONFIG = synthid_mixin.DEFAULT_WATERMARKING_CONFIG\n",
        "CONFIG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "79mekKj5UUZR",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Initialize the required constants, tokenizer, and logits processor\n",
        "\n",
        "BATCH_SIZE = 8\n",
        "NUM_BATCHES = 320\n",
        "OUTPUTS_LEN = 1024\n",
        "TEMPERATURE = 0.5\n",
        "TOP_K = 40\n",
        "TOP_P = 0.99\n",
        "\n",
        "tokenizer = transformers.AutoTokenizer.from_pretrained(MODEL_NAME.value)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"left\"\n",
        "\n",
        "logits_processor = logits_processing.SynthIDLogitsProcessor(\n",
        "    **CONFIG, top_k=TOP_K, temperature=TEMPERATURE\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hndT3YCQUt6D",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Utility functions to load models and process prompts.\n",
        "\n",
        "\n",
        "def load_model(\n",
        "    model_name: ModelName,\n",
        "    expected_device: torch.device,\n",
        "    enable_watermarking: bool = False,\n",
        ") -> transformers.PreTrainedModel:\n",
        "  if model_name == ModelName.GPT2:\n",
        "    model_cls = (\n",
        "        synthid_mixin.SynthIDGPT2LMHeadModel\n",
        "        if enable_watermarking\n",
        "        else transformers.GPT2LMHeadModel\n",
        "    )\n",
        "    model = model_cls.from_pretrained(model_name.value)\n",
        "    model.generation_config.pad_token_id = model.generation_config.eos_token_id\n",
        "  else:\n",
        "    model_cls = (\n",
        "        synthid_mixin.SynthIDGemmaForCausalLM\n",
        "        if enable_watermarking\n",
        "        else transformers.GemmaForCausalLM\n",
        "    )\n",
        "    model = model_cls.from_pretrained(\n",
        "        model_name.value,\n",
        "        #device_map='auto',\n",
        "        torch_dtype=torch.bfloat16,\n",
        "    )\n",
        "\n",
        "  model.to(expected_device)\n",
        "\n",
        "  if str(model.device) != str(expected_device):\n",
        "    raise ValueError('Model device not as expected.')\n",
        "\n",
        "  return model\n",
        "\n",
        "def _process_raw_prompt(prompt: Sequence[str]) -> str:\n",
        "  \"\"\"Add chat template to the raw prompt.\"\"\"\n",
        "  if MODEL_NAME == ModelName.GPT2:\n",
        "    return prompt.decode().strip('\"')\n",
        "  else:\n",
        "    return tokenizer.apply_chat_template(\n",
        "        [{'role': 'user', 'content': prompt.decode().strip('\"')}],\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=True,\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A75tE6NTtcuX"
      },
      "source": [
        "#2. Removing watermark by using LLMs to paraphrase texts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bmyy8uzhtoTY",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Compute g-values of custom texts\n",
        "\n",
        "def generate_responses(input):\n",
        "  inputs = tokenizer(\n",
        "      input,\n",
        "      return_tensors='pt',\n",
        "      padding=True,\n",
        "  ).to(DEVICE)\n",
        "  # Access the correct input_ids\n",
        "  outputs = inputs['input_ids']\n",
        "  # eos mask is computed, skip first ngram_len - 1 tokens\n",
        "  # eos_mask will be of shape [batch_size, output_len]\n",
        "  eos_token_mask = logits_processor.compute_eos_token_mask(\n",
        "      input_ids=outputs,\n",
        "      eos_token_id=tokenizer.eos_token_id,\n",
        "  )[:, CONFIG['ngram_len'] - 1:]\n",
        "\n",
        "  # context repetition mask is computed\n",
        "  context_repetition_mask = logits_processor.compute_context_repetition_mask(\n",
        "      input_ids=outputs,\n",
        "  )\n",
        "  # context repitition mask shape [batch_size, output_len - (ngram_len - 1)]\n",
        "\n",
        "  combined_mask = context_repetition_mask * eos_token_mask\n",
        "\n",
        "  g_values = logits_processor.compute_g_values(\n",
        "      input_ids=outputs,\n",
        "  )\n",
        "  # g values shape [batch_size, output_len - (ngram_len - 1), depth]\n",
        "\n",
        "  return g_values, combined_mask\n",
        "\n",
        "def return_wm_mean_scores(input):\n",
        "  wm_g_values, wm_mask = generate_responses(\n",
        "      input\n",
        "  )\n",
        "  wm_weighted_mean_scores = detector_mean.weighted_mean_score(\n",
        "      wm_g_values.cpu().numpy(), wm_mask.cpu().numpy()\n",
        "  )\n",
        "  print('Weighted Mean scores for watermarked responses: ', wm_weighted_mean_scores)\n",
        "  return wm_weighted_mean_scores\n",
        "\n",
        "def return_uwm_mean_scores(input):\n",
        "  wm_g_values, wm_mask = generate_responses(\n",
        "      input\n",
        "  )\n",
        "  wm_weighted_mean_scores = detector_mean.weighted_mean_score(\n",
        "      wm_g_values.cpu().numpy(), wm_mask.cpu().numpy()\n",
        "  )\n",
        "  print('Weighted Mean scores for unwatermarked responses: ', wm_weighted_mean_scores)\n",
        "  return wm_weighted_mean_scores\n",
        "\n",
        "def return_paraphrased_mean_scores(input):\n",
        "  wm_modified_g_values, wm_modified_mask = generate_responses(\n",
        "      input\n",
        "  )\n",
        "  wm_modified_weighted_mean_scores = detector_mean.weighted_mean_score(\n",
        "      wm_modified_g_values.cpu().numpy(), wm_modified_mask.cpu().numpy()\n",
        "  )\n",
        "  print('Weighted Mean scores for paraphrased watermarked responses: ', wm_modified_weighted_mean_scores)\n",
        "  return wm_modified_weighted_mean_scores\n",
        "\n",
        "def detector(input):\n",
        "  if return_paraphrased_mean_scores(input) > 0.4985:\n",
        "    return True\n",
        "  else:\n",
        "    return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "XK_RLEgO4lGW"
      },
      "outputs": [],
      "source": [
        "#@title Convert markdown to text\n",
        "from bs4 import BeautifulSoup\n",
        "from markdown import markdown\n",
        "import re\n",
        "\n",
        "def markdown_to_text(markdown_string):\n",
        "    \"\"\" Converts a markdown string to plaintext \"\"\"\n",
        "\n",
        "    # md -> html -> text since BeautifulSoup can extract text cleanly\n",
        "    html = markdown(markdown_string)\n",
        "\n",
        "    # remove code snippets\n",
        "    html = re.sub(r'<pre>(.*?)</pre>', ' ', html)\n",
        "    html = re.sub(r'<code>(.*?)</code >', ' ', html)\n",
        "\n",
        "    # extract text\n",
        "    soup = BeautifulSoup(html, \"html.parser\")\n",
        "    text = ''.join(soup.findAll(text=True))\n",
        "\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "6MZltLIltiZd",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Generate watermarked and unwatermarked output\n",
        "\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "def generate_output(example_inputs, enable_watermarking):\n",
        "  inputs = tokenizer(\n",
        "      example_inputs,\n",
        "      return_tensors='pt',\n",
        "      padding=True,\n",
        "  ).to(DEVICE)\n",
        "\n",
        "  model = load_model(MODEL_NAME, expected_device=DEVICE, enable_watermarking=enable_watermarking)\n",
        "\n",
        "  torch.manual_seed(0)\n",
        "  outputs = model.generate(\n",
        "      **inputs,\n",
        "      do_sample=True,\n",
        "      temperature=0.86,\n",
        "      max_length=1024,\n",
        "      top_k=40,\n",
        "  )\n",
        "\n",
        "  lst = []\n",
        "  for i, output in enumerate(outputs):\n",
        "    print(tokenizer.decode(output, skip_special_tokens=True))\n",
        "    lst.append(tokenizer.decode(output, skip_special_tokens=True))\n",
        "\n",
        "  del inputs, outputs, model\n",
        "  return lst\n",
        "\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "source": [
        "#@title Paraphrasing via LLMs\n",
        "model_list = [\"gemma3:4b\", \"llama3.1:8b\" ,\"mistral:7b\", \"phi3:3.8b\", \"qwen3:4b\" ]\n",
        "def paraphrase(input_prompt, model_size):\n",
        "  for i in range(model_size):\n",
        "    # Pass the entire input text as a variable to the prompt template\n",
        "    template = \"\"\"Please paraphrase the following text. Put a symbol '@' at the beginning of the text:\n",
        "    {input_text}\n",
        "    Answer:\"\"\"\n",
        "\n",
        "    prompt = ChatPromptTemplate.from_template(template)\n",
        "    model = OllamaLLM(model=model_list[i])\n",
        "    chain = prompt | model\n",
        "\n",
        "    x = markdown_to_text(Markdown(chain.invoke({\"input_text\": input_prompt[0]})).data).split(\"@\")\n",
        "    input_prompt = [x[-1]]\n",
        "  return input_prompt\n",
        "\n",
        "def run_ollama_serve():\n",
        "  subprocess.Popen([\"ollama\", \"serve\"])\n",
        "\n",
        "thread = threading.Thread(target=run_ollama_serve)\n",
        "thread.start()\n",
        "time.sleep(5)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "I4htnRKDd6aZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Pulling llms api\n",
        "!ollama pull gemma3:4b\n",
        "!ollama pull phi3:3.8b\n",
        "!ollama pull qwen3:4b\n",
        "!ollama pull llama3.1:8b\n",
        "!ollama pull mistral:7b"
      ],
      "metadata": {
        "id": "cMYKGKDyfGPc",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RuJJXjPnuOXz",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Inputting texts for paraphrasing\n",
        "\n",
        "print(\"Input your text here: \")\n",
        "text = input()\n",
        "model_size = int(input(\"Choose your desired model size (1, 3, or 5): \"))\n",
        "\n",
        "#Keep LLMs API running\n",
        "thread = threading.Thread(target=run_ollama_serve)\n",
        "thread.start()\n",
        "time.sleep(5)\n",
        "\n",
        "#Paraphrase the wm text\n",
        "while True:\n",
        "  try:\n",
        "    paraphrased_wm_output = paraphrase([text], model_size)\n",
        "    break\n",
        "  except RuntimeError:\n",
        "    continue\n",
        "\n",
        "print(\"Here is the paraphrased version:\")\n",
        "print(paraphrased_wm_output[0])\n",
        "score = return_paraphrased_mean_scores(paraphrased_wm_output)[0]\n",
        "print()\n",
        "if (score < 0.5189775):\n",
        "  print(\"You are safe\")\n",
        "else:\n",
        "  print(\"You need another try\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Plotting(for determining the threshold)\n",
        "\n"
      ],
      "metadata": {
        "id": "Erdm1FRSoMl5"
      }
    },
    {
      "source": [
        "#@title Plot histogram and normal distribution pdf for UWM and WM scores\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "from scipy.stats import norm\n",
        "import statistics\n",
        "\n",
        "drive.mount('/mnt/drive')\n",
        "\n",
        "result = pd.read_excel('/mnt/drive/MyDrive/Output and Results.xlsx')\n",
        "\n",
        "data = result['UWM Score']\n",
        "# Access the underlying numpy array from the pandas Series\n",
        "x_axis = data.values\n",
        "\n",
        "# Calculating mean and standard deviation\n",
        "mean = statistics.mean(x_axis)\n",
        "sd = statistics.stdev(x_axis)\n",
        "\n",
        "# Create a smooth range of x-values for plotting the PDF\n",
        "# Use linspace to create 100 evenly spaced points between the min and max of your data\n",
        "x_smooth = np.linspace(min(x_axis), max(x_axis), 100)\n",
        "\n",
        "# Calculate the PDF values for the smooth x-values\n",
        "pdf_values = norm.pdf(x_smooth, mean, sd)\n",
        "\n",
        "# Plot the smooth PDF curve\n",
        "plt.plot(x_smooth, pdf_values, label=f'Normal Distribution (Mean: {mean:.4f}, Std Dev: {sd:.4f})')\n",
        "\n",
        "# Optionally, plot a histogram of the actual data for comparison\n",
        "plt.hist(x_axis, bins=20, density=True, alpha=0.6, label='UWM Score Data')\n",
        "\n",
        "plt.xlabel('UWM Score')\n",
        "plt.ylabel('Density')\n",
        "plt.title('Distribution of WM Scores vs. Normal Distribution PDF')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "data = result['WM Score']\n",
        "# Access the underlying numpy array from the pandas Series\n",
        "x_axis = data.values\n",
        "\n",
        "# Calculating mean and standard deviation\n",
        "mean = statistics.mean(x_axis)\n",
        "sd = statistics.stdev(x_axis)\n",
        "\n",
        "# Create a smooth range of x-values for plotting the PDF\n",
        "# Use linspace to create 100 evenly spaced points between the min and max of your data\n",
        "x_smooth = np.linspace(min(x_axis), max(x_axis), 100)\n",
        "\n",
        "# Calculate the PDF values for the smooth x-values\n",
        "pdf_values = norm.pdf(x_smooth, mean, sd)\n",
        "\n",
        "# Plot the smooth PDF curve\n",
        "plt.plot(x_smooth, pdf_values, label=f'Normal Distribution (Mean: {mean:.4f}, Std Dev: {sd:.4f})')\n",
        "\n",
        "# Optionally, plot a histogram of the actual data for comparison\n",
        "plt.hist(x_axis, bins=20, density=True, alpha=0.6, label='WM Score Data', color = 'b')\n",
        "\n",
        "plt.xlabel('WM Score')\n",
        "plt.ylabel('Density')\n",
        "plt.title('Distribution of WM Scores vs. Normal Distribution PDF')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "3LiDct8oUP7v",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Plot 2 normal distribution pdf for UWM and WM\n",
        "\n",
        "data = result['UWM Score']\n",
        "x_axis = data.values\n",
        "mean = statistics.mean(x_axis)\n",
        "sd = statistics.stdev(x_axis)\n",
        "x_smooth = np.linspace(min(x_axis), max(x_axis), 100)\n",
        "pdf_values_uwm = norm.pdf(x_smooth, mean, sd)\n",
        "plt.plot(x_smooth, pdf_values_uwm, label=f'UWM Normal Distribution (Mean: {mean:.4f}, Std Dev: {sd:.4f})')\n",
        "\n",
        "data = result['WM Score']\n",
        "x_axis = data.values\n",
        "mean = statistics.mean(x_axis)\n",
        "sd = statistics.stdev(x_axis)\n",
        "x_smooth = np.linspace(min(x_axis), max(x_axis), 100)\n",
        "pdf_values_wm = norm.pdf(x_smooth, mean, sd)\n",
        "\n",
        "plt.plot(x_smooth, pdf_values_wm, label=f'WM Normal Distribution (Mean: {mean:.4f}, Std Dev: {sd:.4f})')\n",
        "plt.xlabel('Score')\n",
        "plt.ylabel('Density')\n",
        "plt.title('Normal Distribution PDF of WM scores and UWM scores')\n",
        "plt.legend()\n",
        "\n",
        "idx = np.argwhere(np.diff(np.sign(pdf_values_uwm - pdf_values_wm))).flatten()\n",
        "print(f\"Threshold: {x_axis[idx][-1]}\")"
      ],
      "metadata": {
        "id": "ZIWnHeXTXlk-",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "cPzN6_SInzyG",
        "Zt_7oe63n-1U"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}